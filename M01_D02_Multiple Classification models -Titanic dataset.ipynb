{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85fe074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0afed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch    Fare  Embarked_C  Embarked_Q  \\\n",
       "0         0       3    0   2.0      3      2  27.900           0           0   \n",
       "1         1       2    0  13.0      0      1  19.500           0           0   \n",
       "2         0       3    1  30.0      0      0   7.225           1           0   \n",
       "3         0       3    1  25.0      0      0   7.225           1           0   \n",
       "4         0       3    0  18.0      1      0  17.800           0           0   \n",
       "\n",
       "   Embarked_S  \n",
       "0           1  \n",
       "1           1  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('titanic_processed1.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5357bbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pclass',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Fare',\n",
       " 'Embarked_C',\n",
       " 'Embarked_Q',\n",
       " 'Embarked_S']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets extract features \n",
    "\n",
    "FEATURES = list(titanic.columns[1:])#Survived columns is our label\n",
    "FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0f19bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will evaluate each model's accuracy, precision and recall score of test data and store in a dictionary \n",
    "#The keys of the this dictionay is different model we built and values will contain evaluation metrics\n",
    "\n",
    "result_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5b15fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets setup few helper function that we can reuse in our notebook\n",
    "#Takes actual and predicted y value and quickly calculate scores\n",
    "\n",
    "def summarize_classification(y_test, y_pred):\n",
    "    acc = accuracy_score(y_test, y_pred, normalize = True) #normalize=True - accuracy in terms of fraction\n",
    "    num_acc = accuracy_score(y_test, y_pred, normalize = False)#normalize=False - no of accurately predicted label \n",
    "    \n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    return{'accuracy' : acc,\n",
    "           'precision' : prec,\n",
    "           'recall' : recall,\n",
    "           'accuracy_count' : num_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c32567ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another helper function to help us build and train the different classification models in this notebook\n",
    "\n",
    "def build_model(classifier_fn, name_of_y_col, name_of_x_cols, dataset, test_frac=0.2):\n",
    "    \n",
    "    #classifier_fn-function we'll define, takes training data training label instantiates and estimator object and trains the model\n",
    "    #name_of_y_col -string name of the target label.i.e. name of y column\n",
    "    #name_of_x_cols - features.i.e name of the x columns in the form of list\n",
    "    #dataset- dataframe holds our training data\n",
    "    #test_frac - testing fraction \n",
    "    \n",
    "    X = dataset[name_of_x_cols]\n",
    "    Y = dataset[name_of_y_col]\n",
    "    \n",
    "    #Train test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = test_frac)\n",
    "    \n",
    "    #Classifier fucntion\n",
    "    model = classifier_fn(x_train, y_train)\n",
    "    \n",
    "    #Testing prediction\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    #Training prediction\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    \n",
    "    #Scores\n",
    "    train_summary = summarize_classification(y_train, y_pred_train)\n",
    "    test_summary = summarize_classification(y_test, y_pred)\n",
    "    \n",
    "    #Actual vs prediction results in df\n",
    "    pred_results = pd.DataFrame({'y_test' : y_test, 'y_pred' : y_pred})\n",
    "    \n",
    "    #Confusion matrix\n",
    "    model_crosstab = pd.crosstab(pred_results.y_pred, pred_results.y_test)\n",
    "    \n",
    "    return {'training' : train_summary,\n",
    "            'test' : test_summary,\n",
    "            'confusrion_matrix' : model_crosstab}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4309e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function which allow us quickly compare results of the different calssification model that we built\n",
    "\n",
    "def compare_results():\n",
    "    for key in result_dict:\n",
    "        print('Classification : ', key)\n",
    "        \n",
    "        print()\n",
    "        print('Training Data')\n",
    "        for score in result_dict[key]['training']:\n",
    "            print(score, result_dict[key]['training'][score])\n",
    "        print()\n",
    "        print('Test Data')\n",
    "        for score in result_dict[key]['test']:\n",
    "            print(score, result_dict[key]['test'][score])\n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cfc1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll define a function that takes training data and training labels \n",
    "#This instantiates logistic regression estimator and calls fit on the estimator to start training\n",
    "\n",
    "def logistic_fn(x_train, y_train):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9319045d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets call our reuseable function \n",
    "\n",
    "result_dict['survived - logistic'] = build_model(logistic_fn, 'Survived', FEATURES, titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8612fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'survived - logistic': {'training': {'accuracy': 0.789103690685413,\n",
       "   'precision': 0.7674418604651163,\n",
       "   'recall': 0.7021276595744681,\n",
       "   'accuracy_count': 449},\n",
       "  'test': {'accuracy': 0.8321678321678322,\n",
       "   'precision': 0.8536585365853658,\n",
       "   'recall': 0.660377358490566,\n",
       "   'accuracy_count': 119},\n",
       "  'confusrion_matrix': y_test   0   1\n",
       "  y_pred        \n",
       "  0       84  18\n",
       "  1        6  35}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a22c66",
   "metadata": {},
   "source": [
    "<h2>Linear Discriminant Analysis</h2>\n",
    "\n",
    "Linear discriminant isany line that can be used to separate the two classes into which we are categorizing data. Find axes to best separate the classes such that all instances of a class are in the same quadrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcd7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a classifier fucntion takes training data and training labels and instantiates an estimator for\n",
    "#Linear Discriminant Analysis from scikit-learn\n",
    "\n",
    "#Our training data is numeric and every record can be a point  in an N-dimensional space.\n",
    "#The best axes is refer to those axes that the best separate the data into different classes\n",
    "\n",
    "def linear_discriminant_fn(x_train, y_train, solver='svd'):\n",
    "    \n",
    "#Solver='svd' - singular value decomposition solver is default solver, \n",
    "#svd- find axes without calculating the covariance matrix of features useful when we have many features our \n",
    "#training data and many records as well \n",
    "\n",
    "    model = LinearDiscriminantAnalysis(solver=solver)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd38f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7621359223300971\n",
      "recall 0.7008928571428571\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8251748251748252\n",
      "precision 0.8823529411764706\n",
      "recall 0.703125\n",
      "accuracy_count 118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived - linear_discriminant_analysis'] = build_model(linear_discriminant_fn,\n",
    "                                                                    'Survived',\n",
    "                                                                     FEATURES,\n",
    "                                                                     titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9434cd",
   "metadata": {},
   "source": [
    "For many machine learning models if you include all of the columns from your one-hot encoded features in your training data, you'll encounter something  that is called a dummy trap. This occurs when there is a perfect colinearity between two or more features in your training set.\n",
    "\n",
    "This dummy trap can result  in poor ML models. The way to fix this is to use dummy encoding of our categorical variables instead of one-hot encoding, this can be done easily by simply droppingg one of the columns from our one-hot encoded set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24512f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7819905213270142\n",
      "recall 0.7112068965517241\n",
      "accuracy_count 456\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.6896551724137931\n",
      "recall 0.7142857142857143\n",
      "accuracy_count 109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using dummy encoding \n",
    "#Instead of using all of our features from our training data lets drop last columns-one of one-hot encoded column\n",
    "result_dict['survived - linear_discriminant_analysis'] = build_model(linear_discriminant_fn,\n",
    "                                                                    'Survived',\n",
    "                                                                     FEATURES[0:-1],\n",
    "                                                                     titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05c2c3",
   "metadata": {},
   "source": [
    "<h2>Quadratic Discriminant Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a338644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful when the X variable  corresponding to different labels have different covariances.\n",
    "#i.e. covariances are different  for X for all values of Y\n",
    "\n",
    "def quadratic_discriminany_fn(x_train, y_train):\n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4842af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7819905213270142\n",
      "recall 0.7112068965517241\n",
      "accuracy_count 456\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.6896551724137931\n",
      "recall 0.7142857142857143\n",
      "accuracy_count 109\n",
      "\n",
      "Classification :  survived - quadratic_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7843137254901961\n",
      "recall 0.6896551724137931\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.82\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived - quadratic_discriminant_analysis'] = build_model(quadratic_discriminany_fn,\n",
    "                                                                       'Survived',\n",
    "                                                                       FEATURES[0:-1],\n",
    "                                                                       titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f50107",
   "metadata": {},
   "source": [
    "<h2> Stochastic Gradient Descent </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9469b11",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) performs a numeric optimization - one training instance at one time to find the best model parameter. Each training instance is fed into the model at an epoch or an interation, so you have to run sevaral iterations to improve the model.<br>\n",
    "\n",
    "You can specify different hyperparameters to all of these classification models and these are design factor in your model, two parameter for SGD classifier max iter - no of iteration  and tol - tolarance value - stopping criterion for the model training<br>\n",
    "\n",
    "When we specify value the model will stop training if the loss calculated at a particular iteration is less than the tolerance that we have specified as compared with the previous iteration  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4525a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_fn(x_train, y_train, max_iter = 10000, tol = 1e-3):\n",
    "    \n",
    "    model = SGDClassifier(max_iter = max_iter, tol=tol)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69b5c2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7819905213270142\n",
      "recall 0.7112068965517241\n",
      "accuracy_count 456\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.6896551724137931\n",
      "recall 0.7142857142857143\n",
      "accuracy_count 109\n",
      "\n",
      "Classification :  survived - quadratic_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7843137254901961\n",
      "recall 0.6896551724137931\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.82\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 119\n",
      "\n",
      "Classification :  survived - sgd\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6942003514938488\n",
      "precision 0.7938144329896907\n",
      "recall 0.3333333333333333\n",
      "accuracy_count 395\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7342657342657343\n",
      "precision 0.88\n",
      "recall 0.38596491228070173\n",
      "accuracy_count 105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived - sgd'] = build_model(sgd_fn, 'Survived', FEATURES, titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746bcc5",
   "metadata": {},
   "source": [
    "<h1> Support Vector Machine </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c529bf",
   "metadata": {},
   "source": [
    "Find a hyperplane that separates points so all points on the same side belongs to the same class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7392a02",
   "metadata": {},
   "source": [
    "C - inverse of the regularization strength, smaller values indicates stronger regularization and this is to penalize  more complex model- penalize points on the wrong side of the margin.<br>\n",
    "\n",
    "Tolerance is what we use the model training should be stopped,If the calculated loss for two consecutive iteration of model is less than the tolerance value the model will be basically infer that additional training isn't really improving the model parameters by much and it'll stop training.<br>\n",
    "\n",
    "We use LinearSVC estimator object to perform support vector machine classification, There is another estimator object available in scikit-learn that you can use to perform SVM tha tis the SVC object.\n",
    "\n",
    "LinearSVC = SVC(kernal='linear')<br>\n",
    "\n",
    "dual = False is also refer to the optimizatino problem when you perform optimizations in machine learning its possible to convert what is called primal problem to dual problem- dual probelm is easier to solve using optimization<br>\n",
    "\n",
    "Prefer dual = False when n_samples > n_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb11e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svc_fn(x_train, y_train, C=1.0, max_iter = 1000, tol = 1e-3):\n",
    "    \n",
    "    model = LinearSVC(C = C, max_iter = max_iter, tol = tol, dual = False)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "107c7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7819905213270142\n",
      "recall 0.7112068965517241\n",
      "accuracy_count 456\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.6896551724137931\n",
      "recall 0.7142857142857143\n",
      "accuracy_count 109\n",
      "\n",
      "Classification :  survived - quadratic_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7843137254901961\n",
      "recall 0.6896551724137931\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.82\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 119\n",
      "\n",
      "Classification :  survived - sgd\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6942003514938488\n",
      "precision 0.7938144329896907\n",
      "recall 0.3333333333333333\n",
      "accuracy_count 395\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7342657342657343\n",
      "precision 0.88\n",
      "recall 0.38596491228070173\n",
      "accuracy_count 105\n",
      "\n",
      "Classification :  survived - linear_svc\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.780952380952381\n",
      "recall 0.7068965517241379\n",
      "accuracy_count 455\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7902097902097902\n",
      "precision 0.7321428571428571\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived - linear_svc'] = build_model(linear_svc_fn, 'Survived', FEATURES, titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e802e",
   "metadata": {},
   "source": [
    "<h1> Nearest Neighbors </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28dc91",
   "metadata": {},
   "source": [
    "<h3>K - Nearest Neighbors<br>\n",
    "    \n",
    "Radius Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68ed6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create function for nearest neighbors\n",
    "\n",
    "def radius_neighbor_fn(x_train, y_train, radius=40.0):\n",
    "    \n",
    "    #points within the radius are considered neighbors and can vote\n",
    "    \n",
    "    model = RadiusNeighborsClassifier(radius = radius)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8d073f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7819905213270142\n",
      "recall 0.7112068965517241\n",
      "accuracy_count 456\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.6896551724137931\n",
      "recall 0.7142857142857143\n",
      "accuracy_count 109\n",
      "\n",
      "Classification :  survived - quadratic_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7843137254901961\n",
      "recall 0.6896551724137931\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.82\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 119\n",
      "\n",
      "Classification :  survived - sgd\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6942003514938488\n",
      "precision 0.7938144329896907\n",
      "recall 0.3333333333333333\n",
      "accuracy_count 395\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7342657342657343\n",
      "precision 0.88\n",
      "recall 0.38596491228070173\n",
      "accuracy_count 105\n",
      "\n",
      "Classification :  survived - linear_svc\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.780952380952381\n",
      "recall 0.7068965517241379\n",
      "accuracy_count 455\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7902097902097902\n",
      "precision 0.7321428571428571\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 113\n",
      "\n",
      "Classification :  survived - radius_neighbors\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6783831282952548\n",
      "precision 0.7469879518072289\n",
      "recall 0.2767857142857143\n",
      "accuracy_count 386\n",
      "\n",
      "Test Data\n",
      "accuracy 0.6293706293706294\n",
      "precision 0.6774193548387096\n",
      "recall 0.328125\n",
      "accuracy_count 90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived - radius_neighbors'] = build_model(radius_neighbor_fn,\n",
    "                                                         'Survived', \n",
    "                                                         FEATURES, \n",
    "                                                         titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b30f12",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e18eb",
   "metadata": {},
   "source": [
    "Fit a decision tree to training data using CART-(Classification And Regression Tree) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fd3475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_fn(x_train, y_train, max_depth=None, max_features = None):\n",
    "    \n",
    "    model = DecisionTreeClassifier( max_depth = max_depth, max_features = max_features )\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aabf54a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7819905213270142\n",
      "recall 0.7112068965517241\n",
      "accuracy_count 456\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.6896551724137931\n",
      "recall 0.7142857142857143\n",
      "accuracy_count 109\n",
      "\n",
      "Classification :  survived - quadratic_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7843137254901961\n",
      "recall 0.6896551724137931\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.82\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 119\n",
      "\n",
      "Classification :  survived - sgd\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6942003514938488\n",
      "precision 0.7938144329896907\n",
      "recall 0.3333333333333333\n",
      "accuracy_count 395\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7342657342657343\n",
      "precision 0.88\n",
      "recall 0.38596491228070173\n",
      "accuracy_count 105\n",
      "\n",
      "Classification :  survived - linear_svc\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.780952380952381\n",
      "recall 0.7068965517241379\n",
      "accuracy_count 455\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7902097902097902\n",
      "precision 0.7321428571428571\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 113\n",
      "\n",
      "Classification :  survived - radius_neighbors\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6783831282952548\n",
      "precision 0.7469879518072289\n",
      "recall 0.2767857142857143\n",
      "accuracy_count 386\n",
      "\n",
      "Test Data\n",
      "accuracy 0.6293706293706294\n",
      "precision 0.6774193548387096\n",
      "recall 0.328125\n",
      "accuracy_count 90\n",
      "\n",
      "Classification :  survived - decison_tree\n",
      "\n",
      "Training Data\n",
      "accuracy 0.9859402460456942\n",
      "precision 1.0\n",
      "recall 0.9655172413793104\n",
      "accuracy_count 561\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8041958041958042\n",
      "precision 0.7413793103448276\n",
      "recall 0.7678571428571429\n",
      "accuracy_count 115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived - decison_tree'] = build_model(decision_tree_fn, 'Survived', FEATURES, titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7ae59",
   "metadata": {},
   "source": [
    "# Naive Bayes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b45d7",
   "metadata": {},
   "source": [
    "Naive Bayes' makes naive (strong) assumption about independence of features, Use Bayes' theorem to find which label is most likely , given the attributes observed  in teh features vector, and given how often the different labels occur in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29468a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_fn(x_train, y_train, priors=None):\n",
    "    \n",
    "    #Priors probabilities of the classes, when not specified the priors are adjusted based on the data\n",
    "    \n",
    "    model = GaussianNB(priors = priors)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    #Different features in our dataset are considered to be independent \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3414b0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification :  survived - logistic\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7783505154639175\n",
      "recall 0.6741071428571429\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7972027972027972\n",
      "precision 0.8070175438596491\n",
      "recall 0.71875\n",
      "accuracy_count 114\n",
      "\n",
      "Classification :  survived - linear_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.8014059753954306\n",
      "precision 0.7819905213270142\n",
      "recall 0.7112068965517241\n",
      "accuracy_count 456\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7622377622377622\n",
      "precision 0.6896551724137931\n",
      "recall 0.7142857142857143\n",
      "accuracy_count 109\n",
      "\n",
      "Classification :  survived - quadratic_discriminant_analysis\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7961335676625659\n",
      "precision 0.7843137254901961\n",
      "recall 0.6896551724137931\n",
      "accuracy_count 453\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8321678321678322\n",
      "precision 0.82\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 119\n",
      "\n",
      "Classification :  survived - sgd\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6942003514938488\n",
      "precision 0.7938144329896907\n",
      "recall 0.3333333333333333\n",
      "accuracy_count 395\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7342657342657343\n",
      "precision 0.88\n",
      "recall 0.38596491228070173\n",
      "accuracy_count 105\n",
      "\n",
      "Classification :  survived - linear_svc\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7996485061511424\n",
      "precision 0.780952380952381\n",
      "recall 0.7068965517241379\n",
      "accuracy_count 455\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7902097902097902\n",
      "precision 0.7321428571428571\n",
      "recall 0.7321428571428571\n",
      "accuracy_count 113\n",
      "\n",
      "Classification :  survived - radius_neighbors\n",
      "\n",
      "Training Data\n",
      "accuracy 0.6783831282952548\n",
      "precision 0.7469879518072289\n",
      "recall 0.2767857142857143\n",
      "accuracy_count 386\n",
      "\n",
      "Test Data\n",
      "accuracy 0.6293706293706294\n",
      "precision 0.6774193548387096\n",
      "recall 0.328125\n",
      "accuracy_count 90\n",
      "\n",
      "Classification :  survived - decison_tree\n",
      "\n",
      "Training Data\n",
      "accuracy 0.9859402460456942\n",
      "precision 1.0\n",
      "recall 0.9655172413793104\n",
      "accuracy_count 561\n",
      "\n",
      "Test Data\n",
      "accuracy 0.8041958041958042\n",
      "precision 0.7413793103448276\n",
      "recall 0.7678571428571429\n",
      "accuracy_count 115\n",
      "\n",
      "Classification :  survived - naive_bayes\n",
      "\n",
      "Training Data\n",
      "accuracy 0.7750439367311072\n",
      "precision 0.7512953367875648\n",
      "recall 0.6444444444444445\n",
      "accuracy_count 441\n",
      "\n",
      "Test Data\n",
      "accuracy 0.7832167832167832\n",
      "precision 0.8333333333333334\n",
      "recall 0.6349206349206349\n",
      "accuracy_count 112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_dict['survived - naive_bayes'] = build_model(naive_bayes_fn, 'Survived', FEATURES, titanic)\n",
    "\n",
    "compare_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97b718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
